{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9YyQnE2faTU"
   },
   "source": [
    "# Topic Modeling and Classification of Customer Complaints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ignmGVdLjfo_"
   },
   "source": [
    "## **Table of Contents** \n",
    "\n",
    "1. **Problem Statement** \n",
    "\n",
    "\n",
    "2. **Essential Comments**\n",
    "\n",
    "\n",
    "3. **Import Libraries and Modules** \n",
    "\n",
    "\n",
    "4. **Load JSON Data as Pandas DataFrame** \n",
    "\n",
    "\n",
    "5. **Exploratory Data Analysis and Data Preprocessing**\n",
    "\n",
    "    - 5.1 Data Exploration \n",
    "    - 5.2 Data Exploration Summary\n",
    "    - 5.3 Data Preprocessing\n",
    "    - 5.4 Visualization of Preprocessed Data\n",
    "\n",
    "\n",
    "6. **Topic Modeling with TF-IDF Vectorization and NMF**\n",
    "    \n",
    "    - 6.1 Vectorize Raw Texts to TF-IDF Feature Matrix\n",
    "    - 6.2 Find Optimal Number of Topic with NMF\n",
    "    - 6.3 Manual Topic Modeling with NMF\n",
    "\n",
    "\n",
    "7. **Build Supervised Model to Classify New Complaints**\n",
    "    \n",
    "    - 7.1 Predictive Classifier 1: Multinomial Naive Bayes\n",
    "    - 7.2 Predictive Classifier 2: Logistic Regression\n",
    "    - 7.3 Predictive Classifier 3: Decision Tree\n",
    "    - 7.4 Predictive Classifier 4: Random Forest\n",
    "\n",
    "\n",
    "8. **Conclusions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "nQaL6dQ7jfk0"
   },
   "source": [
    "## **1. Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "zPF0S7gHfoGV"
   },
   "source": [
    "Build a model that can classify customer complaints based on the product/service. This will allow the segreggation of these complaints (or tickets) to their relevant categories (or topics), thereby helping in the quick resolution of an issue.\n",
    "\n",
    "\n",
    "We will be doing topic modeling on a consumer complaints data set. Since the data is not labeled, we will be applying the non-negative matrix factorization (**NMF**) approach for topic modeling of consumer complaints and clustering them into one of the following five categories:\n",
    "\n",
    "- **Credit/Prepaid Card**\n",
    "- **Bank Account Services**\n",
    "- **Theft/Dispute Reporting**\n",
    "- **Mortgages/Loans**\n",
    "- **Others**\n",
    "\n",
    "With the aid of topic modeling, we will be able to map each ticket onto the respective department/category. We will then use this data to train any classifier such as *logistic regression*, *decision tree*, or *random forest*. Finally, using the trained classifier we will classify any new customer complaint support ticket to the relevant department. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "LwqK2NyDudE8"
   },
   "source": [
    "## **2. Essential Comments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "U9mRazP9uZmZ"
   },
   "source": [
    "1. If you are running this notebook on Google Colab, uncomment the first cell in the section below, called **\"Import Required Libraries and Modules\"**, and run the notebook end-to-end.\n",
    "\n",
    "2. If you are running this notebook on a local or virtual machine, make sure to create a new virtual or `conda` environment, install all required libraies, and then run the notebook end-to-end. \n",
    "\n",
    "3. Make sure that you use `gensim==4.0` package if you want to use the `nmf` model available in `gensim.models`. The `NMF.py` file is not available in `gensim.models` with old versions of gensim such as `gensim==3.6` or `gensim==3.8`.\n",
    "\n",
    "4. The most time consuming parts of this notebook are the lemmatization step present in \"**Exploratory Data Analysis and Data Preprocessing**\" and the NMF step in **\"Topic Modeling with TF-IDF Vectorization and NMF\"**. One can in principle, decrease this time with either/both the following approaches:\n",
    "\n",
    "    - The **stemming** approach (from `nltk` package) of wrapping the **lemmatization** step with other data preprocessing steps into a `spaCy` **pipeline**. \n",
    "\n",
    "    - Using a subset of the \"non-empty\" (\"non-blank\") consumer complaints instead of the whole data set of around 22,000 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "fZ4xsi2xupci"
   },
   "source": [
    "## **3. Import Required Libraries and Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "0DMMh473ZHZ4",
    "outputId": "67febfb0-0a61-4802-fe3f-861d96f7436f"
   },
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm\n",
    "!python -m textblob.download_corpora\n",
    "!python3 -m pip install gensim==4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "N3iWwwa1uSI-",
    "outputId": "3dff2556-95f0-4356-e930-b49d956e6c68"
   },
   "outputs": [],
   "source": [
    "# Builtin libraries\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import IPython as ipy\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "# Third-party libraries for data science\n",
    "# and machine learning \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl \n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import sklearn as skl \n",
    "\n",
    "# Third-party NLP libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import textblob\n",
    "import wordcloud\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "y9B9svTuvU7N",
    "outputId": "07c0c2f3-3d6b-46d3-ee27-076880b2975d"
   },
   "outputs": [],
   "source": [
    "print(f'{\"re --version\":{20}} : {re.__version__:s}')\n",
    "print(f'{\"json --version\":{20}} : {json.__version__:s}')\n",
    "print(f'{\"nltk --version\":{20}} : {nltk.__version__:s}')\n",
    "print(f'{\"spacy --version\":{20}} : {spacy.__version__:s}')\n",
    "print(f'{\"ipython --version\":{20}} : {ipy.__version__:s}')\n",
    "print(f'{\"numpy --version\":{20}} : {np.__version__:s}')\n",
    "print(f'{\"pandas --version\":{20}} : {pd.__version__:s}')\n",
    "print(f'{\"matplotlib --version\":{20}} : {mpl.__version__:s}')\n",
    "print(f'{\"seaborn --version\":{20}} : {sns.__version__:s}')\n",
    "print(f'{\"plotly --version\":{20}} : {plotly.__version__:s}')\n",
    "print(f'{\"sklearn --version\":{20}} : {skl.__version__:s}')\n",
    "print(f'{\"textblob --version\":{20}} : {textblob.__version__:s}')\n",
    "print(f'{\"wordcloud --version\":{20}} : {wordcloud.__version__:s}')\n",
    "print(f'{\"gensim --version\":{20}} : {gensim.__version__:s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "yaEC4evXuyu8"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "EzyDJqByvL6O"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer \n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn import decomposition as decomp\n",
    "\n",
    "from plotly import offline as plot\n",
    "from plotly import graph_objects as go\n",
    "from plotly import express as px\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import nmf, CoherenceModel \n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "fvLraf2lZ_LV"
   },
   "outputs": [],
   "source": [
    "nlp_model = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "On7oVtBHx1Qv"
   },
   "source": [
    "## **4. Load JSON Data as Pandas DataFrame** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "6Lk8ZQZbyS7W",
    "outputId": "57f9fc64-6d4d-48d2-be42-00d6ca255a6a"
   },
   "outputs": [],
   "source": [
    "!pwd \n",
    "!wget -nv https://raw.githubusercontent.com/rs2pydev/nlp_1_CLFwTM/main/data/Client_data.json\n",
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "S21PhkG_vb5c"
   },
   "outputs": [],
   "source": [
    "with open(\"./Client_data.json\") as f_handle:\n",
    "    json_data = json.load(f_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "FVK-PagryE6N"
   },
   "outputs": [],
   "source": [
    "df = pd.json_normalize(json_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "jP-JGSl_zLAd"
   },
   "source": [
    "## **5. Exploratory Data Analysis and Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "Bclotpcv9-N7"
   },
   "source": [
    "### 5.1 Data Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "hidden": true,
    "id": "PfG7fg8_zTtS",
    "outputId": "95eef6d1-2f59-46f5-8d27-efd8b28d2d1f"
   },
   "outputs": [],
   "source": [
    "# Display dataset\n",
    "display(df.sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "JOxnZmkmzdCT",
    "outputId": "a6631786-7026-435c-a7ef-bc9d391ff1c2"
   },
   "outputs": [],
   "source": [
    "# Check number of rows and columns in dataset\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "_NuVA7DGzejq",
    "outputId": "0b53b0cc-2e36-4f07-dc89-433846252696"
   },
   "outputs": [],
   "source": [
    "# Variables' information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "hidden": true,
    "id": "ZwxmmGSTznOI",
    "outputId": "6eb4888c-3c92-40c1-9d4d-7077890ee256"
   },
   "outputs": [],
   "source": [
    "# Identify and collect null columns\n",
    "null_cols = [var for var in df.columns if df[var].isnull().sum() > 0]\n",
    "print(*null_cols, sep='\\n', end='\\n\\n')\n",
    "display(df[null_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "hidden": true,
    "id": "7e_rnsrIcqOc",
    "outputId": "e9e1b308-57aa-47f2-d1f6-9edad864d42e"
   },
   "outputs": [],
   "source": [
    "# Identify and collect non-null columns\n",
    "not_null_cols = [var for var in df.columns if df[var].isnull().sum() == 0]\n",
    "print(*not_null_cols, sep='\\n', end='\\n\\n')\n",
    "display(df[not_null_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "dDO_5z27zpum",
    "outputId": "1224847b-95de-4579-d0f0-5941bcdfda1b"
   },
   "outputs": [],
   "source": [
    "# Create list of column names\n",
    "col_names = df.columns.to_list()\n",
    "print('Column Names: ')\n",
    "print(*col_names, sep=\"\\n\", end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "cGyO6WwPz1J3"
   },
   "outputs": [],
   "source": [
    "def value_count_df(df:pd.DataFrame=None, var:str=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a Pandas DataFrame and a column name, this function displays \n",
    "    the items in the column and their counts (frequencies).\n",
    "    Args:\n",
    "        df: pd.DataFrame | Default value None\n",
    "        var: str | Default value None\n",
    "    Return:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df = df[var].value_counts().reset_index()\n",
    "    new_df.columns = [str(var), 'Count']\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "W4ryBGV43obo",
    "outputId": "67b12fef-f54d-4f49-97fe-1fd2ad4f08b3"
   },
   "outputs": [],
   "source": [
    "# Check distributions of two columns of interest\n",
    "vars = [\n",
    "    '_source.product', \n",
    "    '_source.issue', \n",
    "    '_source.complaint_what_happened'\n",
    "]\n",
    "\n",
    "for var in vars:\n",
    "    tmp = pd.DataFrame()\n",
    "    tmp = value_count_df(df=df, var=var)\n",
    "    print(f'For variable `{var:s}`: ')\n",
    "    display(tmp)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "hidden": true,
    "id": "awD5TCWQ37Mh",
    "outputId": "bd570bde-c3fb-45f1-886f-6602cd9da82b"
   },
   "outputs": [],
   "source": [
    "# We will examine the consumer complaints column to check for\n",
    "# null values hidden as empty strings\n",
    "print('Non-empty items: ')\n",
    "display(df.loc[(df['_source.complaint_what_happened'] != ''), :].shape)\n",
    "print('Empty items: ')\n",
    "display(df.loc[(df['_source.complaint_what_happened'] == ''), :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "tbBWzhs9BKVA"
   },
   "source": [
    "### 5.2 Data Exploration Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "F2o_h4DQh05u"
   },
   "source": [
    "* The dataset has 78313 customer complaints and 22 features with the customer complaint is in `_source.complaint_what_happened` column.\n",
    "\n",
    "* Using the 21072 non-empty (non-blank) rows of the `_source.complaint_what_happened` column, we will create a DataFrame called `df_text`. **NOTE:** 57241 rows of this column are empty (blank). \n",
    "\n",
    "* Next, we rename the `df_text` column.\n",
    "\n",
    "* Finally, we apply text preprocessing (see below) on `df_text.complaints_unclean` and create a new column, `complaints_clean`. \n",
    "    * Convert text to lowercase.\n",
    "    * Remove text in square brackets.\n",
    "    * Remove punctuations.\n",
    "    * Remove words containing numbers.\n",
    "    * Remove all *hidden* words, containing `XXX`\n",
    "    * Use POS tags to get relevant words from the texts - We will use nouns only.\n",
    "    * Lemmatize the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "XZ8EuraSBVjk"
   },
   "source": [
    "### 5.3 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "hidden": true,
    "id": "sFhYULYSj8C5",
    "outputId": "fb58d02c-868d-4e77-8aad-67a733d5ebba"
   },
   "outputs": [],
   "source": [
    "df_text = pd.DataFrame()\n",
    "df_text = pd.DataFrame(df.loc[(df['_source.complaint_what_happened'] != ''), \n",
    "                 '_source.complaint_what_happened']).reset_index(drop=True)\n",
    "df_text.rename(columns={'_source.complaint_what_happened': 'complaints_unclean'}, \n",
    "               inplace=True)\n",
    "display(df_text.sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Sot-WFfMgnZA"
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text:str=None) -> str:\n",
    "    '''\n",
    "    Make text lowercase, remove text in square brackets, remove punctuation \n",
    "    and remove words containing numbers.\n",
    "    Args:\n",
    "        text: str | Default value None\n",
    "    Returns:\n",
    "        str \n",
    "    '''\n",
    "    text = text.lower() # Make word lowercase\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove word in square brackets\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text) # Remove words with digits \n",
    "    text = re.sub(r'x{4}|xx/', '', text) # Remove words with 'XXXX' | 'XX/' \n",
    "    text = re.sub(r'\\n', '', text) # Remove new lines\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', '', text) # Remove all 1-, 2-, and 3-letter words\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text) # Remove punctuations\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "wi-bwDrkDAPI"
   },
   "outputs": [],
   "source": [
    "df_text['complaints_clean'] = df_text['complaints_unclean'].apply(lambda x: text_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "W-CRzZrmkW1n",
    "outputId": "c6f6171b-cfcb-4d67-ccb3-b44a7da54ed4"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "display(df_text.sample(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "-mu_EE1Mnio1"
   },
   "outputs": [],
   "source": [
    "def text_lemmatizer(text:str=None) -> str:        \n",
    "    '''\n",
    "    Function to Lemmatize an input text.\n",
    "    Args:\n",
    "        text: str | Default value None\n",
    "    Returns:\n",
    "        str \n",
    "    '''\n",
    "    lemmas = []\n",
    "    doc = nlp_model(text)\n",
    "    for word in doc:\n",
    "        lemmas.append(word.lemma_)\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "Farkqt5PilHu",
    "outputId": "ae85e935-9d5b-46c1-8d82-64799dfd0d9a"
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe with \n",
    "# --- original (uncleaned) complaints \n",
    "# --- cleaned complaints \n",
    "# --- lemmatized complaints.\n",
    "df_text[\"complaints_lemmatize\"] =  df_text.apply(lambda x: text_lemmatizer(\n",
    "    x['complaints_clean']), axis=1)\n",
    "display(df_text.sample(n=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "t3zOUQtUCH6j"
   },
   "source": [
    "* Unlike verbs and common nouns, there's no clear base form of a personal pronoun.  spaCy's solution is to introduce a novel symbol, -PRON-, which is used as the lemma for all personal pronouns.\n",
    "\n",
    "* **Chunking** in NLP is a process to take small pieces of information and group them into large units. The primary use of **Chunking** is making groups of \"noun phrases. Here we are using only singular nouns as we have already lemmatized the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "weef7E3K8mRM"
   },
   "outputs": [],
   "source": [
    "def pos_tag(text):\n",
    "    try:\n",
    "        return TextBlob(text).tags\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return ' '.join([word for (word,tag) in blob.tags if tag == \"NN\"])\n",
    "\n",
    "df_text[\"complaints_POS_removed\"] =  df_text.apply(lambda x: \n",
    "                                                    get_adjectives(x['complaints_lemmatize']), \n",
    "                                                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "4uqDFzTm4QuA",
    "outputId": "496df768-488d-42f5-b005-9a825fea3d6c"
   },
   "outputs": [],
   "source": [
    "# Now, `df_text` DataFrame contains: \n",
    "# --- Raw (unclean) complaints\n",
    "# --- Cleaned complaints \n",
    "# --- Lemmatized complaints \n",
    "# --- Complaints after removing POS tags.\n",
    "\n",
    "display(df_text.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "8o_nIrJo91Z1"
   },
   "source": [
    "### 5.4 Visualization of Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "hidden": true,
    "id": "k2O2GpJ04xOU",
    "outputId": "7d6371c6-98d8-4203-bd9b-bb5b93328f8e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "doc_lens = [len(d) for d in df_text.complaints_POS_removed]\n",
    "plt.hist(doc_lens, bins = 50)\n",
    "plt.title('Distribution of Complaint character length')\n",
    "plt.ylabel('Number of Complaint')\n",
    "plt.xlabel('Complaint character length')\n",
    "sns.despine()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "s4-aoW1tLiar"
   },
   "source": [
    "The above plot shows that in terms of the distribution of the word counts, it is positively skewed.\n",
    "\n",
    "Below, we show the top 40 words by frequency among all the articles after processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "jewJIhcbMzXa",
    "outputId": "651e9db4-4d8e-48a8-8721-500111c0e671"
   },
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wc = WordCloud(background_color='white', stopwords=stopwords, max_words=40, \n",
    "               max_font_size=40, random_state=42).generate(str(df_text['complaints_POS_removed']))\n",
    "print(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "hidden": true,
    "id": "rDUH88nK49XK",
    "outputId": "6023352a-6e21-4f54-f18b-fc545243a102"
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12.0,12.0)  \n",
    "mpl.rcParams['font.size'] = 12            \n",
    "mpl.rcParams['savefig.dpi'] = 100             \n",
    "mpl.rcParams['figure.subplot.bottom'] =.1 \n",
    "fig = plt.figure()\n",
    "plt.imshow(wc);\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "QuBjCQ-L5BcI",
    "outputId": "4472c482-0a94-4f9d-9bbc-ed62aa2afea9"
   },
   "outputs": [],
   "source": [
    "#Removing `-PRON-` from the text corpus\n",
    "df_text['complaints_fin_ver'] = df_text['complaints_POS_removed'].str.replace('-PRON-', '')\n",
    "display(df_text.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "9w22qESGNVD3"
   },
   "source": [
    "Given below are the top unigrams, bigrams and trigrams by frequency among all the complaints after processing the text: \n",
    "\n",
    "- **credit**\n",
    "- **debt** \n",
    "- **bank** \n",
    "- **loan** \n",
    "- **mortgage** \n",
    "\n",
    "The above are some of the top words which makes sense given the focus of the complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "lo0Yq9fSNzRV"
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "hidden": true,
    "id": "8XMNme7lNNVC",
    "outputId": "f939f099-4e28-4851-8cce-62e7fcbed011"
   },
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(df_text['complaints_fin_ver'].values.astype('U'), 50)\n",
    "df2 = pd.DataFrame(common_words, columns = ['unigram' , 'count'])\n",
    "display(df2.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "hidden": true,
    "id": "6jFKHWNZzyin",
    "outputId": "b6cae431-e0db-407f-ad0a-c6182da9298c"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Bar(x=df2['unigram'], y=df2['count'])])\n",
    "fig.update_layout(title=go.layout.Title(\n",
    "    text=\"Top 50 unigrams in the Complaint text after removing stop words and lemmatization\"))\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Qw5xYbpaOQeg"
   },
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "hidden": true,
    "id": "Qs4tOP3dOtg4",
    "outputId": "45501b84-e0aa-4d24-cc9c-bd7df00b17dc"
   },
   "outputs": [],
   "source": [
    "common_words = get_top_n_bigram(df_text['complaints_fin_ver'].values.astype('U'), 30)\n",
    "df3 = pd.DataFrame(common_words, columns = ['bigram' , 'count'])\n",
    "display(df3.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "hidden": true,
    "id": "xmDIdTeSOtpL",
    "outputId": "350a5415-803f-47b5-c22f-e67b40a4fd67"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Bar(x=df3['bigram'], y=df3['count'])])\n",
    "fig.update_layout(title=go.layout.Title(\n",
    "    text=\"Top 30 bigrams in the Complaint text after removing stop words and lemmatization\"))\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "jhyfO2j_PIp7"
   },
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 774
    },
    "hidden": true,
    "id": "SIT-G7VfPIud",
    "outputId": "8c8076b8-cb4f-4725-b93d-4e1d61eac3db"
   },
   "outputs": [],
   "source": [
    "common_words = get_top_n_trigram(df_text['complaints_fin_ver'].values.astype('U'), 30)\n",
    "df4 = pd.DataFrame(common_words, columns = ['trigram' , 'count'])\n",
    "df4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "hidden": true,
    "id": "ywLKr6y0PptR",
    "outputId": "a29b24b7-5702-43aa-f1d7-96f97a2031f9"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Bar(x=df4['trigram'], y=df4['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 30 trigrams in the Complaint text\"))\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "k6joC0h-f3i-"
   },
   "source": [
    "## **6. Topic Modeling with TF-IDF Vectorization and NMF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "hN38IaHfQDS2"
   },
   "source": [
    "### 6.1 Vectorize Raw Texts to TF-IDF Feature Matrix\n",
    "\n",
    "Here:\n",
    "\n",
    "- `max_df` is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". `max_df = 0.95` means \"ignore terms that appear in more than 95% of the complaints\"\n",
    "\n",
    "- `min_df` is used for removing terms that appear too infrequently. `min_df = 2` means \"ignore terms that appear in less than 2 complaints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "rzGfeoKXPp2y"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "nWuZZ5OSQc33"
   },
   "source": [
    "Create a document term matrix using `fit_transform()`. The contents of a document term matrix are tuples of `(complaint_id,token_id)` TF-IDF score such that those tuples that are absent have a score of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "JWgBdZguQytk"
   },
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(df_text['complaints_fin_ver'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "xX8vpUeZedZo"
   },
   "source": [
    "### 6.2 Find Optimal Number of Topic with NMF \n",
    "\n",
    "The Non-Negative Matrix Factorization (NMF) is an unsupervised technique wherein high dimensional (word) vectors are decomposed (or factorized) into lower-dimensional (lower-rank) representations. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Flx7TJgVe8ds"
   },
   "source": [
    "We will use a **coherence model** to automatically select the best number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "MGsRZbe6eUEq"
   },
   "outputs": [],
   "source": [
    "# Use Gensim's NMF to get the best num of topics via coherence score\n",
    "texts = df_text['complaints_fin_ver']\n",
    "dataset = [d.split() for d in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "eol6rNgGeUTu"
   },
   "outputs": [],
   "source": [
    "# Create a Gensim dictionary, i.e., a mapping between \n",
    "# words and their integer id\n",
    "dictionary = Dictionary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Zn9HMkAkgmnb"
   },
   "outputs": [],
   "source": [
    "# Filter out extremes to limit the number of features\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.85, keep_n=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "zQmnRlHMgmp1"
   },
   "outputs": [],
   "source": [
    "# Create the bag-of-words format => list of tuples with \n",
    "# each tuple being (token_id, token_count)\n",
    "corpus = [dictionary.doc2bow(text) for text in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "ThcmBvIPgmr8"
   },
   "outputs": [],
   "source": [
    "# Create a list of the topic numbers we want to try\n",
    "topic_nums = list(np.arange(5, 10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "-Km4XZYBgmub"
   },
   "outputs": [],
   "source": [
    "# Run the nmf model and calculate the coherence score\n",
    "# for each number of topics\n",
    "coherence_scores = []\n",
    "for num in topic_nums:\n",
    "\n",
    "    NMF = nmf.Nmf(corpus=corpus, num_topics=num, id2word=dictionary, chunksize=2000, \n",
    "              passes=5, kappa=.1, minimum_probability=0.01, w_max_iter=300, \n",
    "              w_stop_condition=0.0001, h_max_iter=100, h_stop_condition=0.001, \n",
    "              eval_every=10, normalize=True, random_state=42)\n",
    "    \n",
    "    # Run the coherence model to get the score\n",
    "    cm = CoherenceModel(model=NMF, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_scores.append(round(cm.get_coherence(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "FpOJgjgTgmxv",
    "outputId": "f6b64704-3ff9-46d8-8383-1b6e9d228364"
   },
   "outputs": [],
   "source": [
    "# Get the number of topics with the highest coherence score\n",
    "scores = list(zip(topic_nums, coherence_scores))\n",
    "best_num_topics = sorted(scores, key=itemgetter(1), reverse=True)[0][0]\n",
    "print(best_num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "14L2V5UnsGgp"
   },
   "source": [
    "### 6.3 Manual Topic Modeling with NMF\n",
    "\n",
    "With the above `CoherenceModel` we got the best number of topics as 5.Now, all we need to do is run the model. The only parameter that is required is the number of components i.e. the number of topics we want. *This is the most crucial part in any topic modeling process and will greatly affect how good your final topics are.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "hAAopXPnsG1m",
    "outputId": "40a036c6-18c6-4cd8-894e-5d5c6d4d3d99"
   },
   "outputs": [],
   "source": [
    "nmf_model = decomp.NMF(n_components=5,random_state=40)\n",
    "nmf_model.fit(dtm)\n",
    "print()\n",
    "print(len(tfidf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "Kav55Sg2sG4F",
    "outputId": "d79d5ad5-94d0-46bc-f828-6c80b6bf11ea"
   },
   "outputs": [],
   "source": [
    "# Print the top word of a sample component\n",
    "single_topic = nmf_model.components_[0]\n",
    "single_topic.argsort()\n",
    "top_word_indices = single_topic.argsort()[-10:]\n",
    "for index in top_word_indices:\n",
    "    print(tfidf.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "12VkQdg6sG6b",
    "outputId": "ce922244-a3ca-4a20-9394-72658d2ea893"
   },
   "outputs": [],
   "source": [
    "# Print Top 15 words for each of the topics\n",
    "for index,topic in enumerate(nmf_model.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "nYLAAwLZs3xH",
    "outputId": "b024d5bb-6823-41d9-e725-c9e050e121a5"
   },
   "outputs": [],
   "source": [
    "# Creating the best topic for each complaint\n",
    "topic_results = nmf_model.transform(dtm)\n",
    "topic_results[0].round(2)\n",
    "topic_results[0].argmax()\n",
    "topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "v7QAp5p9s3qa"
   },
   "outputs": [],
   "source": [
    "# Assign the best topic to each of the complaints in \n",
    "# `Topic` column\n",
    "df_text['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "1xfdEwsCs3ht",
    "outputId": "bafd7548-cb59-4779-bcd0-bbace48901cf"
   },
   "outputs": [],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "MgccUoK1s3J0",
    "outputId": "19fdf35a-ee6b-4c3a-a6c3-b98b05afe0e8"
   },
   "outputs": [],
   "source": [
    "# Print the first 5 complaints for each of the topics\n",
    "df_dc=df_text.groupby('Topic').head(5)\n",
    "df_dc.sort_values('Topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "SgCALxG9t1KE"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of topic names and \n",
    "# topics, i.e., topic number\n",
    "topic_names = {\n",
    "    0: \"Bank Account Services\", \n",
    "    1: \"Credit/Prepaid Card\", \n",
    "    2: \"Others\", \n",
    "    3: \"Theft/Dispute Reporting\", \n",
    "    4: \"Mortgage/Loan\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "At_d7S4nt1Ct"
   },
   "outputs": [],
   "source": [
    "# Replace Topics with Topic Names\n",
    "df_text['Topic'] = df_text['Topic'].map(topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "S7zFGcqJTci1",
    "outputId": "b0a57b9d-2ea6-4f08-a79e-516609368234"
   },
   "outputs": [],
   "source": [
    "display(df_text.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "lQX_qEiYuvm7"
   },
   "source": [
    "## **7. Build Supervised Model to Classify New Complaints** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "_LKh4UgIuvLw"
   },
   "source": [
    "We have analyzed and preprocessed raw text data (consumer complaints) and clustered them into 5 topics using NMF technique. In this section we will use supervised machine learning to classify new consumer complaints to the appropriate topic. \n",
    "\n",
    "Since we will be using supervised learning technique, we have to convert the topic names to numbers as ML algorithms are applicable to numbers *only*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "NCKb8XKYt07t"
   },
   "outputs": [],
   "source": [
    "Topic_names = {\n",
    "    \"Bank Account Services\": 0, \n",
    "    \"Credit/Prepaid Card\": 1, \n",
    "    \"Others\": 2, \n",
    "    \"Theft/Dispute Reporting\": 3, \n",
    "    \"Mortgage/Loan\":4\n",
    "}\n",
    "\n",
    "df_text['Topic'] = df_text['Topic'].map(Topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "wMdAi6Sht00I",
    "outputId": "29a9748a-790f-4415-eee4-6af42dcde63e"
   },
   "outputs": [],
   "source": [
    "display(df_text.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "hidden": true,
    "id": "hDNKc8Hgt0qs",
    "outputId": "910da984-bd7b-4157-da46-64f4fbe0f2bf"
   },
   "outputs": [],
   "source": [
    "train_data = df_text.loc[:, [\"complaints_unclean\", \"Topic\"]]\n",
    "display(train_data.sample(n=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "liCLLuJc0Mca"
   },
   "outputs": [],
   "source": [
    "# Get vector count \n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_data.complaints_unclean)\n",
    "\n",
    "# Save word vector to disk \n",
    "pickle.dump(count_vect.vocabulary_, open(\"count_vector.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "0p4X09C70MFh"
   },
   "outputs": [],
   "source": [
    "# Transform word vector to TF-IDF vector \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Save TF-IDF vector to disk\n",
    "pickle.dump(tfidf_transformer, open(\"tfidf.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "-OLjjlX38fI9"
   },
   "outputs": [],
   "source": [
    "def train_test_evaluate(estimator=None, name=None, \n",
    "                        trainData=None, testData=None, testSize:float=0.3, \n",
    "                        randomState:int=42):\n",
    "    \"\"\"\n",
    "    Perform train-test split, train model, test model, and evaluate model performance.\n",
    "    Args:\n",
    "        estimator:sklearn.estimator class object    | Default value None\n",
    "        name:str                                    | Default value None\n",
    "        trainData:pd.Series or np.array             | Default value None\n",
    "        testData:pd.Series or np.array              | Default value None\n",
    "        testSize:float                              | Default value 0.3\n",
    "        randomState:int                             | Default value 42\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform train-test split with `test_size=0.25`\n",
    "    X_train, X_test, y_train, y_test = tts(\n",
    "        trainData, testData, test_size=testSize, random_state=randomState)\n",
    "\n",
    "    # Create Multinomial Naive Bayes classifier \n",
    "    clf = estimator.fit(X_train, y_train)\n",
    "\n",
    "    # Save model to disk\n",
    "    model_name = name + \".pkl\"\n",
    "    pickle.dump(clf, open(model_name, \"wb\"))\n",
    "    print('Model created and saved to disc!')\n",
    "\n",
    "    # Manual creation of the topic names as a list\n",
    "    target_names = [\n",
    "    \"Bank Account Services\", \n",
    "    \"Credit/Prepaid Card\", \n",
    "    \"Others\", \n",
    "    \"Theft/Dispute Reporting\", \n",
    "    \"Mortgage/Loan\"\n",
    "    ]\n",
    "\n",
    "    docs_new = \"I can not get from chase who services my mortgage, who owns it and who has original loan docs\"\n",
    "    docs_new = docs_new.split(\" \")\n",
    "\n",
    "    # Load model\n",
    "    loaded_vec = CountVectorizer(vocabulary=pickle.load(open(\"count_vector.pkl\", \"rb\")))\n",
    "    loaded_tfidf = pickle.load(open(\"tfidf.pkl\",\"rb\"))\n",
    "    loaded_model = pickle.load(open(model_name,\"rb\"))\n",
    "    print('Model loaded from disc!')\n",
    "\n",
    "    # Test model: Make predictions and evaluate model\n",
    "    X_new_counts = loaded_vec.transform(docs_new)\n",
    "    X_new_tfidf = loaded_tfidf.transform(X_new_counts)\n",
    "    predicted = loaded_model.predict(X_new_tfidf)\n",
    "    # print('Target Names: ', target_names[predicted[:]])\n",
    "    predicted = loaded_model.predict(X_test)\n",
    "    result = pd.DataFrame({\n",
    "        'true_topic': y_test.apply(lambda x: target_names[x]), \n",
    "        'predicted_topic': y_test.apply(lambda x: target_names[x]), \n",
    "        'true_topic_num': y_test, \n",
    "        'predicted_topic_num': predicted\n",
    "        })\n",
    "    display(result.head(10))\n",
    "    print()\n",
    "   \n",
    "    conf_mat = confusion_matrix(y_test,predicted)\n",
    "    print(conf_mat)\n",
    "    print()\n",
    "    clf_report = classification_report(\n",
    "        y_test, predicted, target_names=target_names)\n",
    "    print(clf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "m7zr1GqE1wdR"
   },
   "source": [
    "### 7.1 Predictive Classifier 1: Multinomial Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "hidden": true,
    "id": "-ORbyaDebXOe",
    "outputId": "ca5a31f7-541c-47e9-feda-6e9a573a1991"
   },
   "outputs": [],
   "source": [
    "clf_mB = MultinomialNB()\n",
    "train_test_evaluate(estimator=clf_mB, name='nb_model', \n",
    "                    trainData=X_train_tfidf, testData=train_data.Topic, \n",
    "                    testSize=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "EN_jvYf_XEgG"
   },
   "source": [
    "### 7.2 Predictive Classifier 2: Logistic Regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "hidden": true,
    "id": "KCbzvHTTt0Rl",
    "outputId": "0a314481-02a2-4b1f-f401-5f982a57d422"
   },
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(random_state=0)\n",
    "train_test_evaluate(estimator=clf_lr, name='lr_model',\n",
    "                    trainData=X_train_tfidf, testData=train_data.Topic, \n",
    "                    testSize=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "XlhYJyva5Vqb"
   },
   "source": [
    "### 7.3 Predictive Classifier 3: Decision Tree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "hidden": true,
    "id": "Fek1i4S-0vkl",
    "outputId": "4f996bca-8397-41f6-b99b-4dcc4fc4d38c"
   },
   "outputs": [],
   "source": [
    "clf_dt = DecisionTreeClassifier(random_state=0)\n",
    "train_test_evaluate(estimator=clf_dt, name='dt_model',\n",
    "                    trainData=X_train_tfidf, testData=train_data.Topic, \n",
    "                    testSize=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "jqgFLNdc5uig"
   },
   "source": [
    "### 7.4 Predictive Classifier 4: Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "hidden": true,
    "id": "6qoFIlcV5uA3",
    "outputId": "c532fb0a-1447-4706-9e16-7dab4650022c"
   },
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(random_state=0, max_depth=10)\n",
    "train_test_evaluate(estimator=clf_rf, name='rf_model',\n",
    "                    trainData=X_train_tfidf, testData=train_data.Topic, \n",
    "                    testSize=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "0jXQu5bK6axs"
   },
   "source": [
    "## **8. Conclusions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "K_E9a8TX6nM4"
   },
   "source": [
    "1. Out of all the four models compared, *without any kind of model optimization*, the performance of the Logistic Regression algorithm, in terms of F1 score, precision and recall metrics, is the best for all the 5 topics.\n",
    "\n",
    "2. The *unoptimized* performances of Random Forest and Multinomial Naive Bayes algorithms are comparable, and very poor w.r.t the **Mortgage/Loan** topic.\n",
    "\n",
    "3. The performance of the Decision Tree classifier stands in between that of Logistic Regression algorithm and the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "WTHXQq5q0Bpn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nQaL6dQ7jfk0",
    "LwqK2NyDudE8",
    "fZ4xsi2xupci",
    "On7oVtBHx1Qv",
    "jP-JGSl_zLAd",
    "Bclotpcv9-N7",
    "tbBWzhs9BKVA",
    "XZ8EuraSBVjk",
    "8o_nIrJo91Z1",
    "k6joC0h-f3i-",
    "hN38IaHfQDS2",
    "xX8vpUeZedZo",
    "14L2V5UnsGgp",
    "lQX_qEiYuvm7",
    "EN_jvYf_XEgG",
    "XlhYJyva5Vqb",
    "jqgFLNdc5uig",
    "0jXQu5bK6axs"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
